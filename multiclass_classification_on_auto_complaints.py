# -*- coding: utf-8 -*-
"""Multiclass Classification on Auto Complaints.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gtJ4CC2CXZI5ajXV9HeUg_jDAXhF6fjZ
"""

!pip install transformers

import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import tensorflow as tf
from transformers import BertTokenizer

#Read data
data = pd.read_excel(r'/Complaints_2010_2014 - Copy.xlsx')
data.head()

data.info()

data['PROD_TYPE'].value_counts()

#Initialize the tokenizer from BERT model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

data['CDESCR'].iloc[10]

tokenizer.encode_plus(data['CDESCR'].iloc[0])

X_input_ids = np.zeros((len(data), 512))
X_attn_masks = np.zeros((len(data), 512))

data.shape
X_input_ids.shape
X_attn_masks.shape

#tokenize data
def generate_training_data(data, ids, masks, tokenizer):
  for i, text in tqdm(enumerate(data['CDESCR'])):
    tokenized_text = tokenizer.encode_plus(text, max_length = 512, truncation = True, padding = 'max_length', add_special_tokens= True)
    ids[i, :] = tokenized_text.input_ids
    masks[i, :] = tokenized_text.attention_mask
  return ids, masks

X_input_ids, X_attn_masks = generate_training_data(data, X_input_ids, X_attn_masks, tokenizer )

X_attn_masks

labels = np.zeros((len(data), 10))
labels

#One hot encoding

# Define class label for one-hot encoding
class_labels = {label: i for i, label in enumerate(data['Class'].unique())}

# Map the original class labels to integer values
class_label_mapping = data['Class'].map(class_labels)

# Apply one-hot encoding
labels[np.arange(len(data)), class_label_mapping.values.astype(int)] = 1

labels

#Data creation step using the tensorflow utility function Dataset model
dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks,labels ))

def DatasetMapFunction(input_ids,attn_masks,labels):
  return{
      'input_ids': input_ids,
      'attention_mask': attn_masks
  }, labels

dataset = dataset.map(DatasetMapFunction)

dataset = dataset.shuffle(10000).batch(5, drop_remainder = True)

p = 0.8
train_size = int((len(data)//5)*p)

train_size

#Split train and Validation dataset
train_dataset = dataset.take(train_size)
val_dataset = dataset.skip(train_size)

#Model creation
import tensorflow as tf
from tensorflow.keras import layers
from transformers import TFBertModel
bert_model = TFBertModel.from_pretrained('bert-base-cased')

#Define Model
input_ids = tf.keras.layers.Input(shape = (512,), name = 'input_ids', dtype = 'int32')
attention_masks = tf.keras.layers.Input(shape = (512,), name = 'attention_masks', dtype = 'int32')

bert_embds = bert_model.bert(input_ids, attention_mask = attention_masks )[1]
intermediate_layer = tf.keras.layers.Dense(512, activation = 'relu', name = 'intermediate_layer')(bert_embds)
output_layer = tf.keras.layers.Dense(5, activation = 'softmax', name = 'output_layer')(intermediate_layer)

model = tf.keras.Model(inputs = [input_ids, attention_masks ], outputs = output_layer)
model.summary()

#Define optimizer, loss function, and accuracy
optim = tf.keras.optimizers.Adam(learning_rate = 1e-5)
loss_func = tf.keras.losses.CategoricalCrossentropy()
acc = tf.keras.metrics.CategoricalAccuracy('accuracy')

model.compile(optimizer = optim, loss = loss_func, metrics = [acc])

model.fit(
    train_dataset,
    validation_data = val_dataset,
    epochs = 1
)

import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import tensorflow as tf
from transformers import BertTokenizer
from transformers import TFBertModel

# Read data
data = pd.read_excel(r'/Complaints_2010_2014 - Copy.xlsx')

# Initialize the tokenizer from BERT model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize data
X_input_ids = np.zeros((len(data), 512))
X_attn_masks = np.zeros((len(data), 512))

def generate_training_data(data, ids, masks, tokenizer):
    for i, text in tqdm(enumerate(data['CDESCR'])):
        tokenized_text = tokenizer.encode_plus(
            text,
            max_length=512,
            truncation=True,
            padding='max_length',
            add_special_tokens=True
        )
        ids[i, :] = np.clip(tokenized_text['input_ids'], a_min=None, a_max=tokenizer.model_max_length)
        masks[i, :] = tokenized_text['attention_mask']
    return ids, masks

X_input_ids, X_attn_masks = generate_training_data(data, X_input_ids, X_attn_masks, tokenizer)

# One-hot encoding
class_labels = {label: i for i, label in enumerate(data['Class'].unique())}
class_label_mapping = data['Class'].map(class_labels)
labels = np.zeros((len(data), len(class_labels)))
labels[np.arange(len(data)), class_label_mapping.values.astype(int)] = 1

# Data creation using TensorFlow Dataset
dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))

def DatasetMapFunction(input_ids, attn_masks, labels):
    return {
        'input_ids': input_ids,
        'attention_masks': attn_masks
    }, labels

dataset = dataset.map(DatasetMapFunction)

# Shuffle and split dataset
dataset = dataset.shuffle(10000).batch(5, drop_remainder=True)
p = 0.8
train_size = int((len(data) // 5) * p)
train_dataset = dataset.take(train_size)
val_dataset = dataset.skip(train_size)

# Model creation
bert_model = TFBertModel.from_pretrained('bert-base-cased')

# Define model
input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')
attention_masks = tf.keras.layers.Input(shape=(512,), name='attention_masks', dtype='int32')

bert_outputs = bert_model([input_ids, attention_masks])[1]
intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_outputs)
output_layer = tf.keras.layers.Dense(len(class_labels), activation='softmax', name='output_layer')(intermediate_layer)

model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output_layer)

# Define optimizer, loss function, and accuracy
optim = tf.keras.optimizers.Adam(learning_rate=1e-5)
loss_func = tf.keras.losses.CategoricalCrossentropy()
acc = tf.keras.metrics.CategoricalAccuracy('accuracy')

model.compile(optimizer=optim, loss=loss_func, metrics=[acc])

# Train the model
model.fit(train_dataset, validation_data=val_dataset, epochs=1)
#Save the model
model.save('classification_model')

#Predict the output on the input given by the users
input_text = '1998 TOYOTA COROLLA. ACCELERATED ON ITS OWN.  ON AUGUST 20, 1999, THE CONSUMER COMPLAINED ABOUT A RUBBING SOUND IN THE AREA OF THE RIGHT REAR TIRE WHEN THE BRAKES WERE APPLIED EARLY IN THE MORNINGS. THE MECHANIC COULD NOT FIND ANYTHING WRONG.  THE DOOR PANELS WERE REPLACED DUE TO THE GLUE COMING THROUGH THE FABRIC.  THE DOME LIGHT WOULD NOT COME ON AT TIMES. THE CONSUMER SUSPECTED THERE MAY BE AN ELECTRICAL PROBLEM.  *JB  THE CONSUMER STATED WHILE DRIVING AT A SPEED OF 44 MPH, THE VEHICLE WOULD ACCELERATE AND THEN DECREASE IN SPEED.  THE CONSUMER FIRST NOTICED THE PROBLEM WHEN THE AIR CONDITIONER WAS ON.  UPDATED 05/26/10.*JB'
def prepare_data(input_text, tokenizer):
  for i, text in tqdm(enumerate(data['CDESCR'])):
    token = tokenizer.encode_plus(
        input_text,
        max_length=512,
        truncation=True,
        padding='max_length',
        add_special_tokens=True,
        return_tensors='tf'
     )
    return {
        'input_ids': tf.cast(token.input_ids, tf.float64),
        'attention_mask': tf.cast(token.attention_mask, tf.float64)
    }
tokenized_input_text = prepare_data(input_text,tokenizer )

probs = model.predict(tokenized_input_text)

# Train the model
model.fit(train_dataset, validation_data=val_dataset, epochs=1)
#Save the model
model.save('classification_model')

#Save the model
model.save('classification_model')

loaded_model = tf.keras.models.load_model('classification_model', custom_objects={'Attention': attention_masks})

#Predict the output on the input given by the users
input_text = 'VEHICLE PURCHASED USED WITH 62,000 MILES IN MAY 2006.  FREQUENT & RANDOM DASH WARNING LIGHTS COMING ON & OFF FOR NO REASON--INCLUDING TRANSAXLE, BRAKE, EMISSIONS SYSTEM LIGHT, ETC.  RADIO WILL TURN ON & OFF WITHOUT THE KEYS IN THE VEHICLE.  MECHANIC SAYS THAT THERE WAS SOME MOISTURE IN PCM & THAT PCM  DOES NOT LOOK ORIGINAL.    VEHICLE HAS LOST POWER WITHOUT WARNING WHILE DRIVING SEVERAL TIMES & THEN RECOVERED WITH NO WARNING LIGHTS ON.  TRANSMISSION WENT IN DRIVEWAY WITHOUT WARNING.  NEEDED APPROX. $1500.00 IN REPAIRS TO TRANSMISSION.  MECHANIC SAYS FAILING PART INCLUDES PUMP SHAFT & VEINS.  ALSO RECONDITIONED PUMP SLIDE & SEALS. *TR'

def prepare_data(input_text, tokenizer):
    token = tokenizer.encode_plus(
        input_text,
        max_length=512,
        truncation=True,
        padding='max_length',
        add_special_tokens=True,
        return_tensors='tf'
    )
    return tf.cast(token.input_ids, tf.float64), tf.cast(token.attention_mask, tf.float64)

input_ids, attention_mask = prepare_data(input_text, tokenizer)
probs = model.predict((input_ids, attention_mask))

probs

list(class_labels.keys())