# -*- coding: utf-8 -*-
"""BERT_GPT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w6bKaRBNqLyjcH8hIBvEk6w01fJeTBCo
"""

Import os
import snowflake.connector
import pandas as pd
import numpy as np

OS.environ['http_proxy'] = "http://proxy.fw.com:8080"
OS.environ['https_proxy'] = "http://proxy.fw.com:8080"
OS.environ['no_proxy'] = ".snowflakecomputing.com"

#Call df1 & df2
with open('/home/sgolla1/creds/creds.json','r') as openfile:
  creds = json.load(openfile)
  role_id = ''
  con = snowflake.connector.connect(
      user = creds["ID"]
      password = creds["password"]
      authenticator = "https://didentity.okta.com"
      Role = role_idaccount = "dfs.us-east-1.privatelink"
  )
cursor = con.cursor()
cursor.execute('USE warehouse WHN')
cursor.execute('USE database SFAAP')
query = """

Select * from TB_N

"""
results = cursor.execute(query)
df1 = pd.DataFrame.from_records(iter(results), columns = [x[0] for x in results.description])
print(df1.head())

#NLP libraries
import re
import nltk
from nltk.corpus import stopwords

#Download NLTK stop words
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
Custom_Stop_words = set(['na','a'])
##Combine all stop words
all_stop_words = stop_words.union(Custom_Stop_words)

#Initializing function to eliminate all stop words and preprocess text
def process_text(text):
  text = re.sub(r'[^a-zA-Z\s]', '', text.lower())
  text = ' '.join(word for word in text.split() if word not in all_stop_words)
  return text.strip()

#Process columns of Dataframe
df1['X'] = df1['X'].apply(process_text)
df2['Y'] = df1['Y'].apply(process_text)

#Combine text columns
df1['Text'] = df1['X'] + ' ' + df1['Z']
df2['Text'] = df2['Y'] + ' ' + df2['L']

#Initialize BERT Model
from transformers import BertTokenizer, BertModel
from transformers import GPT2Tokenizer, GPT2Model
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

#Tokenization using BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
def tokenize(text):
  return tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text, add_special_tokens = True)))

#Tokenization using GPT2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
def tokenize(text):
  return tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text, add_special_tokens = True)))

#Apply tokenizer to each row
df1['Tokens'] = df1['Text'].apply(tokenize)
df2['Tokens'] = df2['Text'].apply(tokenize)

#Combine tokenization from both dataframes to create a shared Vocabulary
all_tokens = list(set(df1['Tokens'].sum() + df2['Tokens'].sum()))

#Calculate TF-IDF vectors for each dataframe using shared vocabulary
tfidf_vectorizer = TfidfVectorizer(vocabulary = all_tokens, analyzer = lambda X:X)
tfidf_matrix1 = tfidf_vectorizer.fit_transform(df1['Tokens'])
tfidf_matrix2 = tfidf_vectorizer.fit_transform(df2['Tokens'])

#Calculate cosine similarity scores in a dataframe
cosine_sim = cosine_similarity(tfidf_matrix1, tfidf_matrix2)

#Store similarity scores in a dataframe
similarity_scores = []
for i , row1 in df1.iterrows():
  for j, row2 in df2.iterrows():
    similarity_scores.append({
        'R_ID': row1['R_ID'],
        'RAU_ID': row2['RAU_ID']
        'Similarity_Score': cosine_sim[i, j]
    })
result = pd.DataFrame(similarity_scores)

result.to_excel('FN', index = False)